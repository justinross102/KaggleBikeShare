mu <- mean(num39)
s <- sd(num39)
n <- length(num39)
MoE <- qt(0.95, df = n-1)*s/sqrt(n)
cat(mu - MoE, mu + MoE)
t.test(num39)
cat(mu - MoE, mu + MoE)
mu <- mean(num39)
s <- sd(num39)
n <- length(num39)
MoE <- qt(0.975, df = n-1)*s/sqrt(n)
t.test(num39)
t.test(num39)
cat(mu - MoE, mu + MoE)
num46 <- scan()
mean(num46)
sd(num46)
num46<- scan()
mean(num46)
mu <- 8.48
s <- 0.79
n <- 14
MoE <- qt(0.05, df = n-1, lower.tail = F)*s/sqrt(n)
mu - MoE
mu <- mean(num39)
s <- sd(num39)
n <- length(num39)
MoE <- qt(0.05, df = n-1, lower.tail = F)*s/sqrt(n)
cat(mu - MoE, mu + MoE)
t.test(num39)
mu <- mean(num39)
s <- sd(num39)
n <- length(num39)
MoE <- qt(0.05, df = n-1)*s/sqrt(n)
t.test(num39)
cat(mu - MoE, mu + MoE)
MoE <- qt(0.95, df = n-1)*s/sqrt(n)
t.test(num39)
cat(mu - MoE, mu + MoE)
MoE <- qt(0.975, df = n-1)*s/sqrt(n)
t.test(num39)
cat(mu - MoE, mu + MoE)
num47 <- scan()
mu <- mean(num47)
sd <- sd(num47)
n <- 10
mu <- mean(num47)
s <- sd(num47)
n <- 10
mu <- mean(num47)
s <- sd(num47)
n <- 10
MoE <- qt(0.99, df = n-1, lower.tail = F)*s/sqrt(n)
mu + MoE
mu <- mean(num47)
s <- sd(num47)
n <- 10
MoE <- qt(0.99, df = n-1)*s/sqrt(n)
mu + MoE
mu <- mean(num47)
s <- sd(num47)
n <- 10
MoE <- qt(0.995, df = n-1)*s/sqrt(n)
mu + MoE
mu <- 8.48
s <- 0.79
n <- 14
MoE <- qt(0.975, df = n-1)*s/sqrt(n)
mu - MoE
mu <- mean(num47)
s <- sd(num47)
n <- 10
MoE <- qt(0.995, df = n-1)*s/sqrt(n)
mu + MoE
num49 <- scan()
mu <- mean(num49)
s <- sd(num49)
n <- 10
dof <- n-1
MoE <- qt(0.975, df = dof)*s*sqrt((1/n) + 1)
cat(mu - MoE, mu + MoE)
qchisq(0.995, 2)
qchisq(0.99, 2)
sqrt(qchisq(0.99, 2))
pt(q=-2.3, df=17, lower.tail=TRUE)
pt(q=-2.3, df=17, lower.tail=TRUE)
pt(q=-2.3, df=17)
pt(q=-1.8, df=17)
pt(q=-3.6, df=17)
num66 <- c(0.53/6, 0.65/6, 0.46/6, 0.5/6, 0.37/6)
hist(num66)
library(car)
library(car)
qqPlot(num66)
t.test(num66)
t.test(num66, x = 10, alternative = "less")
t.test(num66, mu = 10, alternative = "less")
t.test(num66, mu = 10, alternative = "less")
t.test(num66, mu = .10, alternative = "less")
num67 <- scan()
mean(num67)
sd(num67)
t.test(num67, mu = 3, alternative = "two.sided")
# setup -------------------------------------------------------------------
library(tidyverse)
library(ggpubr)
library(patchwork)
library(lubridate)
library(gridExtra)
library(bestglm)
library(car)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(corrplot)  # colored correlation matrix
pairs(elec, pch = 19)
num67
# Other Problem 1 ---------------------------------------------------------
num67
sd(num67)
chisq.test((length(num67) - 1) * var(num67) / (1.5)^2, df = length(sample_data) - 1)
chisq.test((length(num67) - 1) * var(num67) / (1.5)^2, df = length(num67) - 1)
chisq.test((length(num67) - 1) * var(num67) / (1.5)^2, df = 29)
chisq.test((length(num67) - 1) * var(num67) / (1.5)^2)
chisq.test(num67)
chisq.test(num67, df=29)
# Set null hypothesis
null_sd <- 1.5
# Degrees of freedom
df <- length(num67) - 1
# Calculate chi-square test statistic
chi_sq <- sum((num67 - mean(num67))^2) / null_sd^2
# Calculate p-value
p_value <- 1 - pchisq(chi_sq, df)
# Print results
cat("Chi-square test statistic:", round(chi_sq, 2), "\n")
cat("Degrees of freedom:", df, "\n")
cat("P-value:", p_value, "\n")
sigma.test(num67, sigma = 1.5, sigmasq = sigma^2, alternative = "greater", conf.level = 0.95)
chi_sq
binnedplot(x = elec$Energy, y = elec$residuals)
# orginial scatterplot
elec %>%
ggplot(mapping = aes(x = MinTemp, y = Energy)) +
geom_point()
pchisq(33.659)
pchisq(33.659, df = 29)
pchisq(33.659, df = 29, lower.tail = F)
names <- c("Autumn", "Justin")
sample(names, 1 replace = T)
sample(names, 1, replace = T)
options <- c("Go tonight!", "Go tomorrow!")
sample(names, 1, replace = T)
sample(names, 1, replace = T)
sample(names, 1, replace = T)
sample(names, 1, replace = T)
options <- c("Go tonight!", "Go tomorrow!")
sample(options, 1 , replace = T)
sample(options, 1 , replace = T)
sample(options, 1 , replace = T)
sample(options, 1 , replace = T)
sample(options, 1 , replace = T)
options <- c("Go tonight!", "Go tomorrow!")
sample(options, 1 , replace = T)
set.seed(1)
options <- c("Go tonight!", "Go tomorrow!")
sample(options, 1 , replace = T)
sample(options, 1 , replace = T)
set.seed(1)
sample(options, 1 , replace = T)
sample(options, 1 , replace = T)
sample(options, 1 , replace = T)
set.seed(1)
install.packages('mlb_pbp')
install.packages('baseballr')
library(baseballr)
bref_standings_on_date()
bref_standings_on_date("2023-06-16", "NL East", from = FALSE)
library(tidyverse)
link <- https://www.costcotravel.com/h=1501
link <- "https://www.costcotravel.com/h=1501"
library(rvest)
get_html_table <- function(url, index, header = T){
df <- url %>%
read_html() %>%
html_elements("table") %>%
html_table(header=header) %>%
.[[index]]
df
}
get_html_table(link,1)
get_html_table("https://www.costcotravel.com/h=1501")
install.packages(tidyverse)
install.packages("tidyverse")
install.packages("rpart")
library(rpart)
setwd("~/Documents/BYU/stat348/KaggleBikeShare/")
# Read in the Data
train <- vroom("./train.csv")
test <- vroom("./test.csv")
# Libraries
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(poissonreg)
library(rpart)
# Read in the Data
train <- vroom("./train.csv")
test <- vroom("./test.csv")
# Remove casual and registered because we can't use them to predict
train <- train %>%
select(-casual, - registered)
log_train <- train %>%
mutate(count=log(count))
my_recipe <- recipe(count~., data=log_train) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("sunny", "mist", "rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("spring", "summer", "fall", "winter"))) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_time(datetime, features="hour") %>%
step_date(datetime, features="dow") %>%
step_date(datetime, features="month") %>%
step_date(datetime, features="year") %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>% # make dummy variables
step_normalize(all_numeric_predictors()) # Make mean 0, sd=1
prepped_recipe <- prep(my_recipe)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(penalized_model)
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Set up grid of tuning values
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
tuning_grid
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- grid_regular(penalty = tune(),    # Specify tuning parameter using tune()
mixture = tune(),
levels = 5) ## L^2 total tuning possibilities
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- expand.grid(
tree_depth = tune(),          # Specify tuning parameter using tune()
cost_complexity = tune(),    # Specify tuning parameter using tune()
min_n = tune()                # Specify tuning parameter using tune()
)
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
rlang::last_trace()
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
## Finalize workflow and predict
final_wf <- penalized_wf %>%
finalize_workflow(bestTune) %>%
fit(data=log_train)
## Set up grid of tuning values
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
tuning_grid
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
folds
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
# Penalized regression model
penalized_model <- linear_reg(penalty=tune(),
mixture=tune()) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
# Set Workflow
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(penalized_model)
# Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
# split data for cross validation
folds <- vfold_cv(log_train, v = 5, repeats = 5)
## Run the CV
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- expand.grid(
tree_depth = tune(),          # Specify tuning parameter using tune()
cost_complexity = tune(),    # Specify tuning parameter using tune()
min_n = tune()                # Specify tuning parameter using tune()
)
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
## Finalize workflow and predict
final_wf <- penalized_wf %>%
finalize_workflow(bestTune) %>%
fit(data=log_train)
## Get Predictions for test set AND format for Kaggle
reg_tree_preds <- predict(final_wf, new_data = test) %>%
mutate(.pred=exp(.pred)) %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count=.pred) %>%
mutate(count=pmax(0, count)) %>%
mutate(datetime=as.character(format(datetime)))
## Finalize workflow and predict
final_wf <- penalized_wf %>%
finalize_workflow(bestTune) %>%
fit(data=log_train)
my_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
penalized_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
## Set up grid of tuning values
tuning_grid <- expand.grid(
tree_depth = tune(),          # Specify tuning parameter using tune()
cost_complexity = tune(),    # Specify tuning parameter using tune()
min_n = tune()                # Specify tuning parameter using tune()
)
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 1)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
## Finalize workflow and predict
final_wf <- penalized_wf %>%
finalize_workflow(bestTune) %>%
fit(data=log_train)
## Set up grid of tuning values
tuning_grid <- expand.grid(
tree_depth = tune(c(5, 10, 15)),          # Specify tuning parameter using tune()
cost_complexity = tune(),    # Specify tuning parameter using tune()
min_n = tune()                # Specify tuning parameter using tune()
)
## Set up grid of tuning values
tuning_grid <- expand.grid(
tree_depth = tune(id = "tree_depth"),          # Specify the id parameter
cost_complexity = tune(id = "cost_complexity"),    # Specify the id parameter
min_n = tune(id = "min_n")                # Specify the id parameter
)
## Set up K-fold CV
folds <- vfold_cv(log_train, v = 5, repeats = 5)
CV_results <- penalized_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTune <- CV_results %>%
select_best("rmse")
## Finalize workflow and predict
final_wf <- penalized_wf %>%
finalize_workflow(bestTune) %>%
fit(data=log_train)
## Get Predictions for test set AND format for Kaggle
reg_tree_preds <- predict(final_wf, new_data = test) %>%
mutate(.pred=exp(.pred)) %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count=.pred) %>%
mutate(count=pmax(0, count)) %>%
mutate(datetime=as.character(format(datetime)))
bestTune
## Finalize workflow and predict
final_wf <- penalized_wf %>%
finalize_workflow(bestTune) %>%
fit(data=log_train)
